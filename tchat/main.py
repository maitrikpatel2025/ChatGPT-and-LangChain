# Import necessary classes from the langchain package and dotenv for environment variables
from langchain.chat_models import ChatOpenAI
from langchain.chains import LLMChain
from langchain.prompts import MessagesPlaceholder, HumanMessagePromptTemplate, ChatPromptTemplate
from langchain.memory import ConversationBufferMemory, FileChatMessageHistory ,ConversationSummaryMemory
from dotenv import load_dotenv

# Load environment variables from a .env file
load_dotenv()

# Initialize a ChatOpenAI object to handle chat-based interactions
chat = ChatOpenAI(verbose=True)

# Set up a memory buffer to store conversation history, with a file-based storage for persistency
# memory = ConversationBufferMemory(
#     chat_memory=FileChatMessageHistory("messages.json"),  # File-based storage for message history
#     memory_key="messages",                                # Key to store messages
#     return_messages=True                                  # Flag to indicate that messages should be returned with the response
# )

# Set up a memory buffer to store conversation history, With summary conversation
memory = ConversationSummaryMemory(
    memory_key="messages",                                # Key to store messages
    return_messages=True,                                  # Flag to indicate that messages should be returned with the response
    llm=chat
)

# Define the prompt structure for chat interactions
prompt = ChatPromptTemplate(
    input_variables=['content', 'messages'],  # Variables used in the template
    messages=[
        MessagesPlaceholder(variable_name="messages"),  # Placeholder for past conversation messages
        HumanMessagePromptTemplate.from_template("{content}")  # Template for the current human message
    ]
)

# Create a chain to process chat interactions using the defined prompt and memory
chain = LLMChain(
    llm=chat,      # The language model (ChatOpenAI) to use
    prompt=prompt, # The prompt template
    memory=memory,  # The conversation memory to use
    verbose=True
)

# Main loop to continuously accept user input and respond
while True:
    content = input(">> ")  # Take input from the user

    result = chain({"content": content})  # Process the input through the chain
    print(result['text'])  # Print the response generated by the chain
