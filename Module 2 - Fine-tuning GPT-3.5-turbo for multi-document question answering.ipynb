{"cells":[{"cell_type":"markdown","metadata":{"id":"I2I2Vpp5bCbF"},"source":["# Module 2 - Fine-tuning GPT-3.5-turbo for multi-document question answering\n","\n","This notebook shows how to fine-tune OpenAI's GPT-3.5-turbo model for multi-document question answering. The model is fine-tuned on the [Incomplete Information Reading Comprehension (IIRC)](https://allenai.org/data/iirc) dataset. The IIRC dataset contains questions that require the model to reason over multiple documents to generate a complete answer. It is a challenging dataset that requires the model to extract relevant information from multiple sources and synthesize it to produce an accurate answer."]},{"cell_type":"markdown","metadata":{"id":"babvaRp4bCbH"},"source":["# Installing required packages\n","\n","\n","In this example, we have to install `openai` and `tiktoken` libraries.\n","\n","**`openai`**:\n","\n","OpenAI is an artificial intelligence research laboratory consisting of the for-profit corporation OpenAI LP and its parent company, the non-profit OpenAI Inc. The OpenAI library is a powerful machine learning library that provides an easy-to-use interface to the OpenAI API. With this library, users can easily integrate OpenAI's state-of-the-art language models, including GPT-3, into their applications, and leverage the full power of these models to perform various natural language processing (NLP) tasks, such as language generation, classification, question-answering, and more.\n","\n","**`tiktoken`**:\n","\n","Tiktoken is an open-source BPE tokenizer developed by OpenAI that is used to split text strings into tokens. It is useful for models like GPT-3 that encode text into tokens. Tiktoken is designed to be highly efficient, capable of handling large amounts of text quickly."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"otw8ybFAbCbI","executionInfo":{"status":"ok","timestamp":1700510625202,"user_tz":180,"elapsed":17893,"user":{"displayName":"Jayr Alencar Pereira","userId":"06138604672424224858"}},"outputId":"95a9d8c6-c873-409a-f877-14d55d9d0e08"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting openai\n","  Downloading openai-1.3.3-py3-none-any.whl (220 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.3/220.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: anyio<4,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n","Collecting httpx<1,>=0.23.0 (from openai)\n","  Downloading httpx-0.25.1-py3-none-any.whl (75 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.13)\n","Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n","Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai) (4.5.0)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (3.4)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.3.0)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.1.3)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n","Collecting httpcore (from httpx<1,>=0.23.0->openai)\n","  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore->httpx<1,>=0.23.0->openai)\n","  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: h11, httpcore, httpx, openai\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","llmx 0.0.15a0 requires cohere, which is not installed.\n","llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed h11-0.14.0 httpcore-1.0.2 httpx-0.25.1 openai-1.3.3\n","Collecting tiktoken\n","  Downloading tiktoken-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n","Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2023.7.22)\n","Installing collected packages: tiktoken\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","llmx 0.0.15a0 requires cohere, which is not installed.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed tiktoken-0.5.1\n"]}],"source":["!pip install openai\n","!pip install tiktoken"]},{"cell_type":"markdown","metadata":{"id":"EskXDu5ObCbJ"},"source":["# Downloading the data\n","\n","To use the Incomplete Information Reading Comprehension (IIRC) dataset as a benchmark, we need to download the data. The IIRC dataset consists of a set of documents and associated questions. We can download the dataset training and validation splits from the [IIRC website](https://allenai.org/data/iirc) and using the following commands:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"66k_n4EJbCbJ","executionInfo":{"status":"ok","timestamp":1700510633298,"user_tz":180,"elapsed":8101,"user":{"displayName":"Jayr Alencar Pereira","userId":"06138604672424224858"}},"outputId":"5cba97f9-9aab-4ee6-9a46-975fc6443932"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2023-11-20 20:03:44--  https://iirc-dataset.s3.us-west-2.amazonaws.com/iirc_train_dev.tgz\n","Resolving iirc-dataset.s3.us-west-2.amazonaws.com (iirc-dataset.s3.us-west-2.amazonaws.com)... 3.5.83.173, 3.5.76.114, 52.92.176.146, ...\n","Connecting to iirc-dataset.s3.us-west-2.amazonaws.com (iirc-dataset.s3.us-west-2.amazonaws.com)|3.5.83.173|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 5713428 (5.4M) [application/gzip]\n","Saving to: ‘iirc_train_dev.tgz’\n","\n","iirc_train_dev.tgz  100%[===================>]   5.45M   989KB/s    in 6.2s    \n","\n","2023-11-20 20:03:51 (893 KB/s) - ‘iirc_train_dev.tgz’ saved [5713428/5713428]\n","\n","._iirc_train_dev\n","iirc_train_dev/\n","iirc_train_dev/._dev.json\n","iirc_train_dev/dev.json\n","iirc_train_dev/._README\n","iirc_train_dev/README\n","iirc_train_dev/._train.json\n","iirc_train_dev/train.json\n"]}],"source":["!wget https://iirc-dataset.s3.us-west-2.amazonaws.com/iirc_train_dev.tgz\n","!tar -xvzf iirc_train_dev.tgz"]},{"cell_type":"markdown","metadata":{"id":"NEHOXv_7bCbJ"},"source":["# Load and prepare data\n","\n","Now, we can load the data and prepare it for training. First, we load the `train.json` file and extract the documents and questions from it."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jLyCf0qJbCbJ"},"outputs":[],"source":["import json\n","\n","train_data = json.load(open(\"./iirc_train_dev/train.json\"))"]},{"cell_type":"markdown","metadata":{"id":"8xoza9ZlbCbK"},"source":["We define the max number of questions we want to use for training. IIRC training set has 10857 questions, but we will use only 1000 questions for training. This is necessary due to the costs for fine-tuning an OpenAI model. You can change the number of questions to use for training by changing the `max_train_questions` variable and check the results on your own responsibility.\n","\n","The code below extract the questions and documents from the training set and stores them in the `train_set_questions`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-feE9Co2bCbK"},"outputs":[],"source":["max_questions = 500 # @param\n","\n","train_set_questions = []\n","i = 0\n","while len(train_set_questions) < max_questions:\n","  item = train_data[i]\n","  for question in item['questions']:\n","    documents = []\n","    for doc in question['context']:\n","      documents.append({\n","          \"title\": doc['passage'] if doc['passage'] != \"main\" else item['title'],\n","          \"content\": doc['text']\n","      })\n","    true_answer = \"\"\n","    if question['answer']['type'] == \"span\":\n","      true_answer = \", \".join([a['text'] for a in question['answer'][\"answer_spans\"]])\n","    elif question['answer']['type'] == \"value\":\n","        true_answer = \"{0} {1}\".format(question['answer']['answer_value'],question['answer']['answer_unit'])\n","    elif question['answer']['type'] == \"binary\":\n","        true_answer = question['answer']['answer_value']\n","    elif question['answer']['type'] == \"none\":\n","        true_answer = \"Not enough information.\"\n","    train_set_questions.append({\n","        \"question\": question['question'],\n","        \"documents\": documents,\n","        \"answer\": true_answer\n","    })\n","  i+=1\n","\n"]},{"cell_type":"markdown","source":["Now, let's prepare the training file. For training chat models with OpenAI's API, we need to provide the model with a list of messages in each example. The code below creates a list of messages for each example in the training set. The list of messages contains the `system` structure, the documents and question as the `user` message, and the answer as the `assistant` message."],"metadata":{"id":"sr_KZPfCmBdG"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"00UKYhSObCbK"},"outputs":[],"source":["file_name = \"messages.jsonl\" # @param\n","all_messages = []\n","\n","with open(file_name, 'w') as outfile:\n","    for example in train_set_questions:\n","        prompt = \"\"\n","        for j, doc in enumerate(example[\"documents\"]):\n","            prompt += f\"[Document {j+1}]: Title: {doc['title']}. Content: {doc['content']}\\n\"\n","            prompt += f\"Question: {example['question']}\"\n","        messages = {\n","            \"messages\": [\n","                {'role':'system', \"content\": \"Your task is to answer a question using the information from the provided documents.\"},\n","                {\"role\":\"user\",\"content\":prompt},\n","                {\"role\":\"assistant\",\"content\":f\"Answer: {example['answer']}\"},\n","            ]\n","        }\n","        all_messages.append(messages)\n","        json.dump(messages, outfile)\n","        outfile.write('\\n')"]},{"cell_type":"markdown","metadata":{"id":"V91m_PFMbCbK"},"source":["# Format validation\n","\n","We can perform a variety of error checks to validate that each conversation in the dataset adheres to the format expected by the fine-tuning API. Errors are categorized based on their nature for easier debugging.\n","\n","1. **Data Type Check**: Checks whether each entry in the dataset is a dictionary (`dict`). Error type: `data_type`.\n","2. **Presence of Message List**: Checks if a `messages` list is present in each entry. Error type: `missing_messages_list`.\n","3. **Message Keys Check**: Validates that each message in the `messages` list contains the keys `role` and `content`. Error type: `message_missing_key`.\n","4. **Unrecognized Keys in Messages**: Logs if a message has keys other than `role`, `content`, and `name`. Error type: `message_unrecognized_key`.\n","5. **Role Validation**: Ensures the `role` is one of \"system\", \"user\", or \"assistant\". Error type: `unrecognized_role`.\n","6. **Content Validation**: Verifies that `content` has textual data and is a string. Error type: `missing_content`.\n","7. **Assistant Message Presence**: Checks that each conversation has at least one message from the assistant. Error type: `example_missing_assistant_message`.\n","\n","The code below performs these checks, and outputs counts for each type of error found are printed. This is useful for debugging and ensuring the dataset is ready for the next steps.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DjOHHBDobCbK","executionInfo":{"status":"ok","timestamp":1700510636417,"user_tz":180,"elapsed":6,"user":{"displayName":"Jayr Alencar Pereira","userId":"06138604672424224858"}},"outputId":"1df1aea3-de72-4344-8121-f79881f206d7"},"outputs":[{"output_type":"stream","name":"stdout","text":["No errors found\n"]}],"source":["from collections import defaultdict\n","\n","format_errors = defaultdict(int)\n","\n","for ex in all_messages:\n","    if not isinstance(ex, dict):\n","        format_errors[\"data_type\"] += 1\n","        continue\n","\n","    messages = ex.get(\"messages\", None)\n","    if not messages:\n","        format_errors[\"missing_messages_list\"] += 1\n","        continue\n","\n","    for message in messages:\n","        if \"role\" not in message or \"content\" not in message:\n","            format_errors[\"message_missing_key\"] += 1\n","\n","        if any(k not in (\"role\", \"content\", \"name\", \"function_call\") for k in message):\n","            format_errors[\"message_unrecognized_key\"] += 1\n","\n","        if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\", \"function\"):\n","            format_errors[\"unrecognized_role\"] += 1\n","\n","        content = message.get(\"content\", None)\n","        function_call = message.get(\"function_call\", None)\n","\n","        if (not content and not function_call) or not isinstance(content, str):\n","            format_errors[\"missing_content\"] += 1\n","\n","    if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n","        format_errors[\"example_missing_assistant_message\"] += 1\n","\n","if format_errors:\n","    print(\"Found errors:\")\n","    for k, v in format_errors.items():\n","        print(f\"{k}: {v}\")\n","else:\n","    print(\"No errors found\")"]},{"cell_type":"markdown","metadata":{"id":"bvCglAjzbCbL"},"source":["# Token Counting Utilities\n","\n","Lets define a few helpful utilities to be used in the rest of the notebook. These utilities will help us count the number of tokens in the dataset and the number of tokens in each example.\n","\n","First, we define the function `num_tokens_from_messages` that counts the number of tokens in a conversation. This function takes a conversation as input and returns the number of tokens in the conversation.\n","\n","The function `num_tokens_from_example` counts the number of tokens in an example. This function takes an example as input and returns the number of tokens in the example.\n","\n","Finally, the function `print_distribution` prints the distribution of the number of tokens in the dataset. This function takes a list of examples as input and prints the distribution of the number of tokens in the dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C2Hw-zxcbCbL"},"outputs":[],"source":["import tiktoken\n","import numpy as np\n","\n","encoding = tiktoken.get_encoding(\"cl100k_base\")\n","\n","# not exact!\n","# simplified from https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n","def num_tokens_from_messages(messages, tokens_per_message=3, tokens_per_name=1):\n","    num_tokens = 0\n","    for message in messages:\n","        num_tokens += tokens_per_message\n","        for key, value in message.items():\n","            num_tokens += len(encoding.encode(value))\n","            if key == \"name\":\n","                num_tokens += tokens_per_name\n","    num_tokens += 3\n","    return num_tokens\n","\n","def num_assistant_tokens_from_messages(messages):\n","    num_tokens = 0\n","    for message in messages:\n","        if message[\"role\"] == \"assistant\":\n","            num_tokens += len(encoding.encode(message[\"content\"]))\n","    return num_tokens\n","\n","def print_distribution(values, name):\n","    print(f\"\\n#### Distribution of {name}:\")\n","    print(f\"min / max: {min(values)}, {max(values)}\")\n","    print(f\"mean / median: {np.mean(values)}, {np.median(values)}\")\n","    print(f\"p5 / p95: {np.quantile(values, 0.1)}, {np.quantile(values, 0.9)}\")"]},{"cell_type":"markdown","metadata":{"id":"w3AjuTx8bCbL"},"source":["# Data Warnings and Token Counts\n","\n","With some lightweight analysis we can identify potential issues in the dataset, like missing messages, and provide statistical insights into message and token counts.\n","\n","1. **Missing System/User Messages**: Counts the number of conversations missing a \"system\" or \"user\" message. Such messages are critical for defining the assistant's behavior and initiating the conversation.\n","2. **Number of Messages Per Example**: Summarizes the distribution of the number of messages in each conversation, providing insight into dialogue complexity.\n","3. **Total Tokens Per Example**: Calculates and summarizes the distribution of the total number of tokens in each conversation. Important for understanding fine-tuning costs.\n","4. **Tokens in Assistant's Messages**: Calculates the number of tokens in the assistant's messages per conversation and summarizes this distribution. Useful for understanding the assistant's verbosity.\n","5. **Token Limit Warnings**: Checks if any examples exceed the maximum token limit (4096 tokens), as such examples will be truncated during fine-tuning, potentially resulting in data loss.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dhgUBh8cbCbL","executionInfo":{"status":"ok","timestamp":1700510639534,"user_tz":180,"elapsed":9,"user":{"displayName":"Jayr Alencar Pereira","userId":"06138604672424224858"}},"outputId":"d30b76b1-20f8-428d-9ce2-0c4b9cd64610"},"outputs":[{"output_type":"stream","name":"stdout","text":["Num examples missing system message: 0\n","Num examples missing user message: 0\n","\n","#### Distribution of num_messages_per_example:\n","min / max: 3, 3\n","mean / median: 3.0, 3.0\n","p5 / p95: 3.0, 3.0\n","\n","#### Distribution of num_total_tokens_per_example:\n","min / max: 72, 593\n","mean / median: 168.91017964071855, 152.0\n","p5 / p95: 92.0, 246.0\n","\n","#### Distribution of num_assistant_tokens_per_example:\n","min / max: 3, 45\n","mean / median: 5.782435129740519, 6.0\n","p5 / p95: 3.0, 7.0\n","\n","0 examples may be over the 4096 token limit, they will be truncated during fine-tuning\n"]}],"source":["# Warnings and tokens counts\n","n_missing_system = 0\n","n_missing_user = 0\n","n_messages = []\n","convo_lens = []\n","assistant_message_lens = []\n","\n","for ex in all_messages:\n","    messages = ex[\"messages\"]\n","    if not any(message[\"role\"] == \"system\" for message in messages):\n","        n_missing_system += 1\n","    if not any(message[\"role\"] == \"user\" for message in messages):\n","        n_missing_user += 1\n","    n_messages.append(len(messages))\n","    convo_lens.append(num_tokens_from_messages(messages))\n","    assistant_message_lens.append(num_assistant_tokens_from_messages(messages))\n","\n","print(\"Num examples missing system message:\", n_missing_system)\n","print(\"Num examples missing user message:\", n_missing_user)\n","print_distribution(n_messages, \"num_messages_per_example\")\n","print_distribution(convo_lens, \"num_total_tokens_per_example\")\n","print_distribution(assistant_message_lens, \"num_assistant_tokens_per_example\")\n","n_too_long = sum(l > 4096 for l in convo_lens)\n","print(f\"\\n{n_too_long} examples may be over the 4096 token limit, they will be truncated during fine-tuning\")"]},{"cell_type":"markdown","metadata":{"id":"6IMO_Cv8bCbM"},"source":["# Cost Estimation\n","\n","In this final section, we estimate the total number of tokens that will be used for fine-tuning, which allows us to approximate the cost. It is worth noting that the duration of the fine-tuning jobs will also increase with the token count."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O0REaxF-bCbM","executionInfo":{"status":"ok","timestamp":1700510639534,"user_tz":180,"elapsed":5,"user":{"displayName":"Jayr Alencar Pereira","userId":"06138604672424224858"}},"outputId":"b4b42415-c224-445b-b7f5-1e10efeed9bc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset has ~84624 tokens that will be charged for during training\n","By default, you'll train for 1 epochs on this dataset\n","By default, you'll be charged for ~84624 tokens\n","By default, you'll be charged ~$0.68 for this dataset\n"]}],"source":["# Pricing and default n_epochs estimate\n","FINETUNING_COST_PER_TOKEN = 0.0080\n","\n","MAX_TOKENS_PER_EXAMPLE = 4096\n","\n","TARGET_EPOCHS = 1\n","MIN_TARGET_EXAMPLES = 100\n","MAX_TARGET_EXAMPLES = 25000\n","MIN_DEFAULT_EPOCHS = 1\n","MAX_DEFAULT_EPOCHS = 25\n","\n","n_epochs = TARGET_EPOCHS\n","n_train_examples = len(all_messages)\n","if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n","    n_epochs = min(MAX_DEFAULT_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\n","elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n","    n_epochs = max(MIN_DEFAULT_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n","\n","n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)\n","print(f\"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\")\n","print(f\"By default, you'll train for {n_epochs} epochs on this dataset\")\n","print(f\"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens\")\n","print(f\"By default, you'll be charged ~${((n_epochs * n_billing_tokens_in_dataset)/1000) * FINETUNING_COST_PER_TOKEN:.2f} for this dataset\")"]},{"cell_type":"markdown","metadata":{"id":"qpVqw2BwbCbM"},"source":["# Fine-tuning\n","\n","Now, let's fine-tune the model using the OpenAI API.\n","\n","First, we retrieve the API key from the Google Colab secrets. See [here](https://medium.com/@parthdasawant/how-to-use-secrets-in-google-colab-450c38e3ec75) how to use secrets in Google Colab. We also instantiate the `openai` library."]},{"cell_type":"code","source":["from google.colab import userdata\n","from openai import OpenAI\n","\n","OPENAI_KEY = userdata.get('openai_api_key')\n","client = OpenAI(api_key=OPENAI_KEY)"],"metadata":{"id":"JtcpjPQlbpAM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gkyrq4LybCbM"},"source":["## Upload training file\n","\n","For fine-tuning, we need to upload the training file to the OpenAI API. The code below uploads the training file to the OpenAI API and stores the information about the uploaded file in the `file_obj` variable. This information will be used later to start the fine-tuning job."]},{"cell_type":"code","source":["file_obj = client.files.create(\n","  file=open(\"messages.jsonl\", \"rb\"),\n","  purpose=\"fine-tune\"\n",")"],"metadata":{"id":"oxBp-kpHb6v0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Create fine-tuning job\n","\n","Now, we create a fine-tuning job using the openai library. The job creation function receives the following parameters:\n","* `model`: The model to fine-tune. In this case, we use the `gpt-3.5-turbo` model. Soon the GPT-4 model will be available.\n","* `training_file`: The id of training file to use for fine-tuning. In this case, we use the `file_obj.id` variable that contains the id of the uploaded training file.\n","* `hyperparameters`: Defines the hyperparameters to use for fine-tuning. In this case, we define only the number of epochs to use for fine-tuning. We use 1 epoch for fine-tuning. You can change the number of epochs to use for fine-tuning by changing the `n_epochs` variable.\n","\n","Once created, the fine-tuning job starts automatically."],"metadata":{"id":"PThgQoVDcMeW"}},{"cell_type":"code","source":["n_epochs = 1 # @param\n","\n","ft_job = client.fine_tuning.jobs.create(\n","  training_file=file_obj.id,\n","  model=\"gpt-3.5-turbo\",\n","  hyperparameters={\n","    \"n_epochs\":n_epochs\n","  }\n",")"],"metadata":{"id":"fJ0BEXa5cL_8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Track fine-tuning job\n","\n","Now, we can track the fine-tuning job. The code below prints the status of the fine-tuning job every 5 seconds. The fine-tuning job is finished when the status is `succeeded`."],"metadata":{"id":"GAQANGANRhNj"}},{"cell_type":"code","source":["from time import sleep\n","message = \"\"\n","while True:\n","  job = client.fine_tuning.jobs.retrieve(ft_job.id)\n","  events = client.fine_tuning.jobs.list_events(ft_job.id)\n","  if job.status == \"succeeded\":\n","    message = events.data[0].message\n","    print(message)\n","    break\n","  if message != events.data[0].message:\n","    message = events.data[0].message\n","    print(message)\n","  sleep(5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hY1JR6uneOSV","executionInfo":{"status":"ok","timestamp":1700511982422,"user_tz":180,"elapsed":1138892,"user":{"displayName":"Jayr Alencar Pereira","userId":"06138604672424224858"}},"outputId":"b8fd1a15-3581-4a7c-f158-4ae3d7b077a2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Validating training file: file-uPByEHOyQXSrFiOEaDZzpC4y\n","Fine-tuning job started\n","Step 1/501: training loss=4.56\n","Step 11/501: training loss=3.41\n","Step 21/501: training loss=1.20\n","Step 31/501: training loss=0.84\n","Step 41/501: training loss=0.77\n","Step 51/501: training loss=0.03\n","Step 61/501: training loss=0.10\n","Step 71/501: training loss=0.45\n","Step 81/501: training loss=0.68\n","Step 91/501: training loss=0.86\n","Step 101/501: training loss=0.04\n","Step 111/501: training loss=0.00\n","Step 121/501: training loss=1.39\n","Step 131/501: training loss=0.00\n","Step 141/501: training loss=0.00\n","Step 151/501: training loss=0.00\n","Step 161/501: training loss=0.00\n","Step 171/501: training loss=2.30\n","Step 181/501: training loss=0.00\n","Step 191/501: training loss=0.00\n","Step 201/501: training loss=0.00\n","Step 211/501: training loss=0.00\n","Step 221/501: training loss=0.06\n","Step 231/501: training loss=0.00\n","Step 241/501: training loss=0.67\n","Step 251/501: training loss=0.01\n","Step 261/501: training loss=0.25\n","Step 271/501: training loss=0.00\n","Step 281/501: training loss=0.00\n","Step 291/501: training loss=2.40\n","Step 301/501: training loss=0.00\n","Step 311/501: training loss=0.00\n","Step 321/501: training loss=3.45\n","Step 331/501: training loss=0.00\n","Step 341/501: training loss=1.75\n","Step 351/501: training loss=0.00\n","Step 361/501: training loss=0.00\n","Step 371/501: training loss=1.29\n","Step 381/501: training loss=0.54\n","Step 391/501: training loss=0.00\n","Step 401/501: training loss=1.35\n","Step 411/501: training loss=0.00\n","Step 421/501: training loss=0.01\n","Step 431/501: training loss=0.00\n","Step 441/501: training loss=0.34\n","Step 451/501: training loss=0.06\n","Step 461/501: training loss=0.00\n","Step 471/501: training loss=1.04\n","Step 481/501: training loss=0.00\n","Step 491/501: training loss=0.01\n","New fine-tuned model created: ft:gpt-3.5-turbo-0613:personal::8N58P9KP\n","The job has successfully completed\n"]}]},{"cell_type":"code","source":["job = client.fine_tuning.jobs.retrieve(ft_job.id)\n","fine_tuned_model = job.fine_tuned_model\n","print(f\"Your fine-tuned model name is '{job.fine_tuned_model}'\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VTtROBoyD1wF","executionInfo":{"status":"ok","timestamp":1700511982423,"user_tz":180,"elapsed":16,"user":{"displayName":"Jayr Alencar Pereira","userId":"06138604672424224858"}},"outputId":"3ed1fdf6-b440-4c77-d2c2-9b07490c5266"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Your fine-tuned model name is 'ft:gpt-3.5-turbo-0613:personal::8N58P9KP'\n"]}]},{"cell_type":"markdown","source":["# Testing the fine-tuned model"],"metadata":{"id":"EitK4ByjFkKk"}},{"cell_type":"markdown","source":["## Downloading the data\n","\n","To use the Incomplete Information Reading Comprehension (IIRC) dataset as a benchmark, we need to download the data. The IIRC dataset consists of a set of documents and associated questions. We can download the dataset test set using the following code:\n","\n"],"metadata":{"id":"KSTZN-_nytrt"}},{"cell_type":"code","source":["!wget https://iirc-dataset.s3.us-west-2.amazonaws.com/iirc_test.json"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2A5abFLoyw1m","executionInfo":{"status":"ok","timestamp":1700511984154,"user_tz":180,"elapsed":1737,"user":{"displayName":"Jayr Alencar Pereira","userId":"06138604672424224858"}},"outputId":"12b0cc44-79a4-4ff0-db9c-35dafa094a4c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2023-11-20 20:26:22--  https://iirc-dataset.s3.us-west-2.amazonaws.com/iirc_test.json\n","Resolving iirc-dataset.s3.us-west-2.amazonaws.com (iirc-dataset.s3.us-west-2.amazonaws.com)... 52.92.194.74, 52.92.161.202, 52.92.224.106, ...\n","Connecting to iirc-dataset.s3.us-west-2.amazonaws.com (iirc-dataset.s3.us-west-2.amazonaws.com)|52.92.194.74|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 2874825 (2.7M) [application/json]\n","Saving to: ‘iirc_test.json’\n","\n","iirc_test.json      100%[===================>]   2.74M  2.93MB/s    in 0.9s    \n","\n","2023-11-20 20:26:23 (2.93 MB/s) - ‘iirc_test.json’ saved [2874825/2874825]\n","\n"]}]},{"cell_type":"markdown","source":["Let's load the data and see what it looks like.\n","\n","We are using the IIRC dataset, and we have imported the JSON library to read the test set file. We loaded the first example from the test set, which is a dictionary with keys 'questions', 'text', 'links', and 'title'.\n","\n","The 'questions' key contains a list of dictionaries with keys 'question', 'context', 'answer', and 'question_links'. The 'text' key contains the text that may contain relevant information for answering the questions. The 'links' key is a list of dictionaries with keys 'target' and 'indices', indicating the hyperlink target and the position of the hyperlink in the text. The 'title' key contains the title of the document.\n","\n","In this particular example, we can see that the question is \"What is Zeus known for in Greek mythology?\" and the answer is \"being the sky and thunder god\". The context contains three passages containing the text that may provide additional information."],"metadata":{"id":"Gm3ivL9yQfqv"}},{"cell_type":"code","source":["import json\n","\n","test_set = json.load(open('iirc_test.json','r'))\n","\n","test_set[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ayTFOsujy0jR","executionInfo":{"status":"ok","timestamp":1700511984155,"user_tz":180,"elapsed":6,"user":{"displayName":"Jayr Alencar Pereira","userId":"06138604672424224858"}},"outputId":"aedd50d7-57c4-4db6-d352-9de6c9410711"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'questions': [{'answer': {'type': 'span',\n","    'answer_spans': [{'text': 'sky and thunder god',\n","      'passage': 'zeus',\n","      'type': 'answer',\n","      'start': 83,\n","      'end': 102}]},\n","   'question': 'What is Zeus know for in Greek mythology?',\n","   'context': [{'text': 'he Palici the sons of Zeus',\n","     'passage': 'main',\n","     'indices': [684, 710]},\n","    {'text': 'in Greek mythology', 'passage': 'main', 'indices': [137, 155]},\n","    {'text': 'Zeus (British English , North American English ; , Zeús ) is the sky and thunder god in ancient Greek religion',\n","     'passage': 'Zeus',\n","     'indices': [0, 110]}],\n","   'question_links': ['Greek mythology', 'Zeus']}],\n"," 'text': \"The Palici (Παλικοί in Greek), or Palaci, were a pair of indigenous Sicilian chthonic deities in Roman mythology, and to a lesser extent in Greek mythology. They are mentioned in Ovid's Metamorphoses V, 406, and in Virgil's Aeneid IX, 585. Their cult centered on three small lakes that emitted sulphurous vapors in the Palagonia plain, and as a result these twin brothers were associated with geysers and the underworld. There was also a shrine to the Palaci in Palacia, where people could subject themselves or others to tests of reliability through divine judgement; passing meant that an oath could be trusted. The mythological lineage of the Palici is uncertain; one legend made the Palici the sons of Zeus, or possibly Hephaestus, by Aetna or Thalia, but another claimed that the Palici were the sons of the Sicilian deity Adranus.\\n\",\n"," 'links': [{'target': 'Sicily', 'indices': [68, 76]},\n","  {'target': 'Chthonic', 'indices': [77, 85]},\n","  {'target': 'Roman mythology', 'indices': [97, 112]},\n","  {'target': 'Greek mythology', 'indices': [140, 155]},\n","  {'target': 'Ovid', 'indices': [179, 183]},\n","  {'target': 'Virgil', 'indices': [215, 221]},\n","  {'target': 'Palagonia', 'indices': [319, 328]},\n","  {'target': 'Geyser', 'indices': [393, 400]},\n","  {'target': 'Zeus', 'indices': [706, 710]},\n","  {'target': 'Hephaestus', 'indices': [724, 734]},\n","  {'target': 'Aetna (nymph)', 'indices': [739, 744]},\n","  {'target': 'Thalia (nymph)', 'indices': [748, 754]},\n","  {'target': 'Adranus', 'indices': [828, 835]}],\n"," 'title': 'Palici'}"]},"metadata":{},"execution_count":16}]},{"cell_type":"markdown","source":["## Data Preparation\n","\n","When working with the IIRC dataset, preparing the data before using it to evaluate models is necessary. Additionally, it is important to carefully choose how many questions to evaluate, as using GPT from OpenAI API can be costly. The data preparation process involves iterating through the test set,\n","extracting relevant information such as the question, documents containing relevant passages, and the true answer.\n","\n","This is achieved by parsing the original JSON format and mapping it into a format that can be used to evaluate models.\n","\n","In the code below, the preparation process is limited to a maximum of 50 questions due to the cost associated with using the OpenAI API. It involves iterating through each question and its corresponding context, creating a list of documents containing relevant information, and extracting the true answer based on its type. By preparing the data in this way, it can be more easily fed into GPT models, and compare the results.\n","\n","We use a limited number of questions to reduce the costs of using OpenAPI API."],"metadata":{"id":"Rp-0LjKV-DjJ"}},{"cell_type":"code","source":["max_questions = 50 # @param\n","\n","test_set_questions = []\n","i = 0\n","while len(test_set_questions) < max_questions:\n","\n","  item = test_set[i]\n","  for question in item['questions']:\n","    documents = []\n","    for doc in question['context']:\n","      documents.append({\n","          \"title\": doc['passage'] if doc['passage'] != \"main\" else item['title'],\n","          \"content\": doc['text']\n","      })\n","    true_answer = \"\"\n","    if question['answer']['type'] == \"span\":\n","      true_answer = \", \".join([a['text'] for a in question['answer'][\"answer_spans\"]])\n","    elif question['answer']['type'] == \"value\":\n","        true_answer = \"{0} {1}\".format(question['answer']['answer_value'],question['answer']['answer_unit'])\n","    elif question['answer']['type'] == \"binary\":\n","        true_answer = question['answer']['answer_value']\n","    elif question['answer']['type'] == \"none\":\n","        true_answer = \"Not enough information.\"\n","    test_set_questions.append({\n","        \"question\": question['question'],\n","        \"documents\": documents,\n","        \"answer\": true_answer\n","    })\n","    i+=1\n"],"metadata":{"id":"ObWhjm-B-GZY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The resulting prepared dataset is a dictionary with four keys. The \"**`question`**\" key contains the actual question to be answered, which in this case is \"What is Zeus known for in Greek mythology?\". The \"**`documents`**\" key is a list containing information that may be relevant to answering the question. Each document is a dictionary with a \"title\" key and a \"content\" key. The \"title\" key gives the name of the source of the information, while the \"content\" key provides the actual text of the source. In this case, there are three documents, all related to the topic of Greek mythology and Zeus. The \"**`answer`**\" key contains the correct answer to the question, which is \"sky and thunder god\".\n","\n"],"metadata":{"id":"8umiH5WOSev1"}},{"cell_type":"code","source":["test_set_questions[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bUAD1vpb-g9F","executionInfo":{"status":"ok","timestamp":1700511984786,"user_tz":180,"elapsed":6,"user":{"displayName":"Jayr Alencar Pereira","userId":"06138604672424224858"}},"outputId":"06d598da-e7a6-4058-a66e-ad40ca60e93d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'question': 'What is Zeus know for in Greek mythology?',\n"," 'documents': [{'title': 'Palici', 'content': 'he Palici the sons of Zeus'},\n","  {'title': 'Palici', 'content': 'in Greek mythology'},\n","  {'title': 'Zeus',\n","   'content': 'Zeus (British English , North American English ; , Zeús ) is the sky and thunder god in ancient Greek religion'}],\n"," 'answer': 'sky and thunder god'}"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["def generate_chat(messages,model=\"gpt-3.5-turbo\"):\n","  response = client.chat.completions.create(\n","    model=model,\n","    messages=messages,\n","    temperature=0\n","  )\n","  return response.choices[0].message.content"],"metadata":{"id":"4Zv6L4RsFzla"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["instruction =  \"Your task is to answer a question using the information from the provided documents.\"\n","def chatgpt_qa(question, documents, examples,model=\"gpt-3.5-turbo\"):\n","  messages = [\n","      {\"role\": \"system\", \"content\": instruction}\n","  ]\n","  # Add the few-shot examples\n","  for example in examples:\n","    prompt = \"\"\n","    for j, doc in enumerate(example[\"documents\"]):\n","      prompt += f\"[Document {j+1}]: Title: {doc['title']}. Content: {doc['content']}\\n\"\n","    prompt += f\"Question: {example['question']}\"\n","\n","    messages += [\n","        {\"role\":\"user\",\"content\":prompt},\n","        {\"role\":\"assistant\", \"content\": f\"Explanation: {example['explanation']}\\nAnswer: {example['answer']}\"}\n","    ]\n","\n","  # Add target example\n","  prompt = \"\"\n","  for k, doc in enumerate(documents):\n","    prompt += f\"[Document {k+1}]: Title: {doc['title']}. Content: {doc['content']}\\n\"\n","  prompt += f\"Question: {question}\"\n","  messages.append({\"role\":\"user\", \"content\": prompt})\n","\n","  res = generate_chat(messages,model=fine_tuned_model) # perform API call\n","  return res.split(\"Answer:\")[1]\n"],"metadata":{"id":"U4rJk44tF6Jk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tqdm import tqdm\n","model = \"gpt-3.5-turbo\" # @param ['gpt-4', 'gpt-4-32k', \"gpt-3.5-turbo\", \"gpt-3.5-turbo-instruct\", \"text-davinci-003\"]\n","k_shot = 3 # @param [1,2,3]\n","for question in tqdm(test_set_questions):\n","\n","  answer = chatgpt_qa(question['question'], question['documents'], [])\n","\n","  question[\"predicted_answer\"] = answer"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ak0nWxJfGS6o","executionInfo":{"status":"ok","timestamp":1700512008960,"user_tz":180,"elapsed":24178,"user":{"displayName":"Jayr Alencar Pereira","userId":"06138604672424224858"}},"outputId":"7de0fc62-7a4c-4037-fa40-bdbe6698cc91"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 52/52 [00:24<00:00,  2.14it/s]\n"]}]},{"cell_type":"markdown","source":["## Evaluation\n","\n","In this section, we evaluate the performance of our model by calculating the exact match score and the F1 score using bag of words. To accomplish this, we have defined some helper functions.\n","\n","The **`normalize_text`** function takes a text and normalizes it by converting it to lowercase and removing any non-alphanumeric characters. The **`get_tokens`** function tokenizes the text after normalization.\n","\n","The **`exact_match`** function takes the predicted answer and the true answer and returns whether they match exactly after normalization. The **`f1_bag_of_words`** function takes the predicted answer and the true answer, tokenizes them, and calculates their F1 score using the bag of words approach.\n","\n","The bag of words approach is a technique used to measure the similarity between two sets of texts by counting the frequency of each word in both sets and then calculating their overlap."],"metadata":{"id":"VCAuOALVLETs"}},{"cell_type":"code","source":["import re\n","from collections import Counter\n","\n","def normalize_text(text):\n","    \"\"\"\n","    Helper function to normalize the text\n","    \"\"\"\n","    text = text.lower()\n","    text = re.sub(r\"[^a-zA-Z0-9]+\", \" \", text)\n","    return text.strip()\n","\n","def get_tokens(text):\n","    \"\"\"\n","    Helper function to tokenize text\n","    \"\"\"\n","    text = normalize_text(text)\n","    return text.split()\n","\n","def exact_match(pred_answer, true_answer):\n","    \"\"\"\n","    Calculates the exact match score\n","    \"\"\"\n","    return normalize_text(pred_answer) == normalize_text(true_answer)\n","\n","def f1_bag_of_words(pred_answer, true_answer):\n","    \"\"\"\n","    Calculates the F1 score using bag of words\n","    \"\"\"\n","    pred_tokens = get_tokens(pred_answer)\n","    true_tokens = get_tokens(true_answer)\n","\n","    pred_counter = Counter(pred_tokens)\n","    true_counter = Counter(true_tokens)\n","\n","    common = pred_counter & true_counter\n","    num_same = sum(common.values())\n","\n","    if num_same == 0:\n","        return 0\n","\n","    precision = 1.0 * num_same / len(pred_tokens)\n","    recall = 1.0 * num_same / len(true_tokens)\n","    f1 = (2 * precision * recall) / (precision + recall)\n","\n","    return f1\n"],"metadata":{"id":"duttlxnNQGLg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In the below code, we are evaluating the performance of the model for multi-document question answering by calculating the: Exact Match (EM) and F1 score using bag of words.\n","\n","The code iterates over each item in the validation dataset and calculates the F1 and EM scores using the **`f1_bag_of_words`** and **`exact_match`** functions defined earlier. The maximum score of F1 and EM is then taken for each item and appended to their respective lists, **`f1s`** and **`ems`**.\n","\n","The mean of the **`f1s`** and **`ems`** lists are then calculated using NumPy's **`np.mean`** function and assigned to **`mean_f1`** and **`mean_em`** variables, respectively. Finally, the average EM and F1 scores are printed using formatted string literals."],"metadata":{"id":"i89xwhKNMhME"}},{"cell_type":"code","source":["import numpy as np\n","\n","f1s, ems = [], []\n","for question in test_set_questions:\n","  if \"predicted_answer\" in question:\n","    f1 = f1_bag_of_words(question[\"predicted_answer\"],question[\"answer\"])\n","    em = exact_match(question[\"predicted_answer\"],question[\"answer\"])\n","    f1s.append(f1)\n","    ems.append(em)\n","\n","mean_em = np.mean(ems)\n","mean_f1 = np.mean(f1s)\n","print(f\"Exact match: {mean_em:.3f}\\nF1-bow: {mean_f1:.3f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G7hrJ86CXQ62","executionInfo":{"status":"ok","timestamp":1700512008960,"user_tz":180,"elapsed":18,"user":{"displayName":"Jayr Alencar Pereira","userId":"06138604672424224858"}},"outputId":"8390598c-9cc1-4611-8981-367e4ebfe626"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Exact match: 0.788\n","F1-bow: 0.833\n"]}]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"colab":{"provenance":[{"file_id":"1nIXFIawOb_B8tjFwpPMr6lZf9FF-fZ-V","timestamp":1700522045221}]}},"nbformat":4,"nbformat_minor":0}