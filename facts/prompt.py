from langchain.vectorstores.chroma import Chroma
from langchain.embeddings import OpenAIEmbeddings
from langchain.chains import RetrievalQA
from langchain.chat_models import ChatOpenAI
from redundant_filter_retriever import RedundantFilterRetriever
from dotenv import load_dotenv

# Load environment variables from a .env file
# This is useful for managing configurations outside the codebase
load_dotenv()

# Initialize embeddings using OpenAI's model
# Embeddings are used for converting text to a vector form that the machine understands
embeddings = OpenAIEmbeddings()

# Initialize a chat model using OpenAI's GPT model
# This model will handle the language generation tasks
chat = ChatOpenAI()

# Initialize Chroma database
# This database stores embeddings and enables efficient similarity searches
db = Chroma(
    persist_directory="emb",
    embedding_function=embeddings,
)

# Initialize a retriever that filters out redundant data
# This is important for reducing repetitive or irrelevant information during retrieval
retriever = RedundantFilterRetriever(
    embeddings=embeddings,
    chroma=db
)

# Set up the RetrievalQA chain
# This chain combines the chat model and retriever for a question-answering task
chain = RetrievalQA.from_chain_type(
    llm=chat,
    retriever=retriever,
    chain_type="stuff"
)

# Run the chain with a sample query and store the result
# The query asks for an interesting fact about the English language
result = chain.run("What is an interesting fact about the English language?")

# Print the result
# This will display the answer generated by the RetrievalQA chain
print(result)
